{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52cdf35c-0813-4282-abdf-76491f7d11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afb6b2ea-2e75-4810-850a-a32420446576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    exp = np.exp(logits - np.max(logits,axis =1,keepdims = True))\n",
    "    return exp / np.sum(exp, aixs=1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aedd8b56-9967-4cc8-a65a-76194e4081c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe(y, num_classes):\n",
    "    m = len(y)\n",
    "    Y = np.zeros((m,num_classes))\n",
    "    Y[np.arrange(m),y]=1\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5af02a26-126b-49ce-80bc-be8eeba1051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(Y,Y_proba, theta, alpha):\n",
    "    m=Y.shape[0]\n",
    "    loss = -np.sum(Y*np.log(Y_proba + 1e-15))/m\n",
    "    reg = alpha * np.sum(theta[1:] ** 2)\n",
    "    return loss +reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2dc589c-e52f-4c6b-bd78-4cd498eead0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_regression_bgd_early_stopping(\n",
    "\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    learning_rate = 0.1,\n",
    "    n_epochs = 1000,\n",
    "    patience =20\n",
    "):\n",
    "\n",
    "    m,n = X_train.shape\n",
    "    X_train_b = np.c_[np.ones((m,1)),X_train]\n",
    "    X_val_b = np.c_[np.ones((X_val.shape[0],1)),X_val]\n",
    "\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    Y_train = ohe(y_train, num_classes)\n",
    "    Y_val = ohe(y_val,num_classes)\n",
    "\n",
    "    theta = np.random.randn(n+1, num_classes)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    best_theta = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        logits = X_train_b @ theta\n",
    "        Y_proba  =softmax(logits)\n",
    "        gradients = (1/m)*X_train_b.T @ (Y_proba - Y_train)\n",
    "        gradients[1:]+=2*alpha*theta[1:]\n",
    "\n",
    "        theta -= eta*gradients\n",
    "\n",
    "        val_logits = X_val_b @ theta\n",
    "        Y_val_proba = softmax(val_logits)\n",
    "        val_loss = cross_entropy_loss(Y_val, Y_val_proba, theta, alpha)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_theta = theta.copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "        return best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82121391-4bdf-49f1-bc3a-56d1f4a791c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    probs = softmax(X_b @ theta)\n",
    "    return np.argmax(probs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dfea0d-c65d-4317-9839-2b9786da31e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
